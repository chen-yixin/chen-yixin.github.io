---
title: CDA Level-II 数据分析模型与应用
categories: CDA
top: false
comments: true
toc: true
date: 2025-08-24
tags:
  - 数据分析
  - 课程笔记
cover: //img.hoboro.top/picgo/202503262106764.jpg?imageView2/0/w/240/h/145
---

CDA > 课程笔记 > Chap5.数据分析模型与应用

<!-- more -->

来源: [CDA level 2 级课堂笔记](https://blog.csdn.net/m0_69037520/article/details/129732340)

## 连续变量降维

### 矩阵分析法（波士顿矩阵/BCG 增长/份额矩阵）

投资组合规划：四要素

- 资源分配
- 拟定业务单位战略
- 制定绩效目标
- 投资组合平衡分析

  - 资金流
  - 可持续性
  - 风险

**1.BCG 矩阵的四种主要战略**

- 增加市场份额
- 维持市场份额
- 收获
- 放弃

### 主成分分析的理论基础

两正态分布变量之间的关系。

变量质量的依赖关系：函数关系、相关关系

主成分分析：目的是构造输入变量的少数线形组合，尽量能解释数据的变异性。这些线形组合被称为主成分，他们形成的降维数据可用于进一步分析。

### 主成分分析的计算步骤

- 每个成分两两之间是正交的。
- 有多少个变量就会有多少个正交的成本。
- 成分的变异（方差）之和等于原始变量的所有变异。
- 前若干个主成分的变异（方差）解释了绝大多数的变异（方差）。
- 如果原始变量不相关，即没有协方差，则不需要做主成分。

**1.主成分个数的选取原则**

- 单个主成分解释的变异不应该小于 1（标准化后比较），比如说选取 3 个主成分，第 3 个主成分解释的变异相当于一个原始变量的变异；
- 选取主成分累计的解释变异达到 80%\~90%。

**2.基于相关系数矩阵的主成分分析**

使用 python 时 X 做标准化，使每个变量均值为 0，方差为 1，这等价于使用相关系数矩阵 R 替代协方差矩阵 Σ 来进行主成分分析。

**3.主成分的解释**

### 主成分分析的应用（维度降维）

**例 1：给出综合评价**

- 步骤一：变量之间相关系数，多数变量之间有明显的强线性相关，这表明做主成分分析是有意义的。

  - 总方差：原始变量总的变异
  - 特征值：每个主成分解释变量的数量
  - 比例：每个特征值解释的变异占原始数据总变异的比例
  - 累计：累计到当前的主成分，总共解释总变异的比例

**例 2：总样本特征描述**

注：如果一个数据的变量可以被压缩成两个主成分，则通过展示在二维图形上已经可以完成样本聚类的工作。如果因子多于两个，则需要使用聚类算法进行样本分类。

### 因子分析的模型设置

应用最广泛的因子旋转方法：

- 是一种正交旋转
- 目的是使载荷平方的方差最大化，即最大化。

描述统计/聚类用因子分析，可解释。做预测模型时使用 主成分分析。

### 因子旋转方法

### 因子分析的应用

### 因子分析与主成分分析的关系

## 多元线性回归

### 客户价值分析框架

客户资产：企业当前客户与潜在客户的货币价值潜力。

客户资产是企业客户终身价值之和（交易价值+推荐价值+成长价值+知识价值）

客户资产 = 单个客户的生命周期价值 ✖ 客户基础

预计未来的客户交易价值

### 线性回归的经典假设

两变量的多元线性回归（参数与 y 形成线性关系，而不是 x 与 y 形成线性关系）

**多元线性回归的假设**

- Y 的平均值能够准确地由 X 组成地线性函数建模出来。
- 解释变量和随机扰动项不存在线性关系（X 与扰动项期望为 0；尽量将与 X/Y 相关的变量放入模型）。
- 解释变量之间不存在线性关系（或强相关；如果强相关则进行降维）。
- 假设随机扰动项是一个均值为 0 的正态分布。
- 假设随机扰动项的方差恒为 σ²。
- 扰动项是独立的。

以上假设可归纳为以下四种假设。

**a.因变量与自变量间的线性关系**

- 模型参数和被解释变量之间是线性关系。
- 解释变量和被解释变量之间可以是任意关系，可以在回归前进行任意函数变换。

**b.正交假定：误差项与自变量不相关，其期望为 0（自己主观推断，没有遗留变量）**

**c.独立同分布：残差间相互独立，且遵循同一分布，要求方差齐性**

**d.正态性：残差服从正态分布**

### 建立线性回归模型的准备

一个有效的线性模型流程：

1. 初始分析
2. 变量选择
3. 验证模型假定（扰动项一般为右偏分布，对 y 取对数转换为正态分布）
4. 多重共线性与强影响点的诊断与分析
5. 模型是否有问题
6. 预测和解释

**诊断统计量方法（判断有没有异常值）**

- 学生化（标准化）残差（绝对值是否大于 2，大于为异常）
- RSTUDENT 残差（学生化残差的优化；去掉异常点后做残差）
- Cook's D
- DFFITS
- DEBETAS

### 建立模型和模型检验

**1.调整后的 R²（Adj.R²）**

**只在模型选择时有用，说明模型解释力度时，还是 R²**

<!-- prettier-ignore-start -->
{% mathjax %}
\bar{R}^{2}=1-\frac{(n-i)(1-R^2)}{n-p} 
{% endmathjax %}
<!-- prettier-ignore-end -->

- i = 1 当有截距项时，反之为 0
- n: 用于拟合该模型的观察值数量
- p: 模型中参数的个数

**2.自变量进入方式**

- 向前法
- 向后法
- 逐步法

决策的指标可为偏回归平方和、AIC/BIC、R 方等。

<!-- prettier-ignore-start -->
{% mathjax %}
AIC=2k+n(log(RSS/n))
{% endmathjax %}
<!-- prettier-ignore-end -->

AIC 越小越好。

共线性检验：膨胀系数法（VIF）

<!-- prettier-ignore-start -->
{% mathjax %}
VI_i=\frac{1}{1-R_i^2}
{% endmathjax %}
<!-- prettier-ignore-end -->

方差膨胀因子>10 表示某变量的多重共线性严重。

**3 线性回归的模型假设：**

1. 模型设置，选择何种回归方法、如选变量、变量以何种形式放入模型（根据理论、看散点图）
2. 解释变量和扰动项不能相关（根据理论或常识判断，无法检验）
3. 解释变量之间不能强线性相关（膨胀系数）
4. 扰动项独立同分布（异方差检验、DW 检验）
5. 扰动项服从正态分布（QQ 检验）

3~5 检验只能保证模型精确，1~2 保证模型是正确的。

- 违反 1，则模型预测能力差
- 违反 2，回归系数估计有偏
- 违反 3，回归系数的标准误被放大
- 违反 4，扰动项的标准差估计不准，T 检验失效
- 违反 5，则 T 检验失效

**4 分类变量作为输入变量（ANOCA 协方差分析）**

spss 只能用广义线性回归（先把连续变量筛选好，再加入至协变量；只要有分类变量，就无法进行分类变量筛选）

### 回归系数的解释

### 利用回归方程进行预测

### 预测性建模与解释性建模的辨析

预测精度与 β 的可解释性。

## 多元逻辑回归模型

### 客户分类评分分析框架

### 似然比和 Logit 变换

### 一元二分类逻辑回归与参数

优势比 odds，小于 1，说明 y 与 x 是负向关系。

### 多元二分类逻辑回归的构建

### 二分类模型的评估

模型评估：成对比较

- 计算一致的对数、不一致的对数以及相等（tied）的对数来评估模型是否很好预测了自身的数据，从而判断模型拟合的是否优秀。
- 通常我们希望一致对的占比高，不一致的和相等对的占比低。

**1.成对比较**

- 一致对（模型预测方向与实际结果一致）
- 不一致对（方向不一致）
- 相等对（模型不能分辨两者，x 是一致的）

**2.ROC 曲线**

**混淆矩阵**

## 聚类模型（连续变量）

### 聚类方法的基本逻辑

- 从 N 个观测和 K 个属性开始
- 计算 N 个观测两两之间的距离
- 将距离相近的观测聚为一类，将距离远的分为不同的类。最终达到组间的距离最大化，组内的聚类最小化

**聚类方法**

- 层次聚类（样本的平方 n²）
  - 形成类似相似度层次图谱，便于直观的确定类之间的划分。该方法可以得到较理想的分类，但是难以处理大量样本。
- 非层次聚类（k ✖ n；k 均值发）
  - 将观测分为预先指定的，不重叠的类。该方法可以处理样本量巨大的数据，但是不能提供类相似度信息，不能交互的决定聚类个数。
- 两步法胡聚类
  - 先使用 k 均值法聚类，然后使用层次方法。

### 系统聚类法

- 建立类之间的层次关系
- 通过层次树决定聚类个数和聚类方法

**1.基本步骤**

- 计算每两个观测之间的距离
- 将最近的两个观测聚类一类，将其看作一个整体计算与其他观测（类）之间的距离
- 一直重复上述过程，直至所有的观测被聚为一类

ward 最小方差法：组内最小离差和

**2.要点 1：要预先处理变量**

收到的数据通常需要处理才能用于分析：

- 缺失值
- 异常值（极大或极小；寻找异常就不需要处理，分群的话需要做秩运算）
- 分类变量需转变为哑变量（0/1 数值）
- 分类变量类别过多

不同的统计方法对数据有不同的要求：

- 决策树允许缺失值和异常值
- 聚类分析和回归模型则不支持缺失值

**要点 2：变量标准化**

- 中心化

<!-- prettier-ignore-start -->
{% mathjax %}
std(x_{ip})=\frac{x_{ip}-\bar{x_p}}{S_p}
{% endmathjax %}
<!-- prettier-ignore-end -->

- 极差标准化

<!-- prettier-ignore-start -->
{% mathjax %}
\frac{x-min(x)}{max(x)-min(x)}
{% endmathjax %}
<!-- prettier-ignore-end -->

**要点 3：不同维度的变量，相关性尽量低**

- 主题相关性（业务上）
- 入模变量间的相关性

### 客户分群分析框架

将现有消费者群体按一定规则划分成若干小群组，使得：

- 每一群组的特征描述丰富详细，不同组之间特征差异明显
- 组内客户特征相似

**1.常见的分群类型**

- 需求和态度
  - 依据调查问卷结果针对需求的数据分群
- 生命周期
  - 依据顾客的消费周期和需求分群
- 行为特征
  - 依据消费记录，个人信息分群
- 客户价值
  - 依据顾客的潜在价值分群

**2.客户分群在商业上的应用**

品牌、媒体、渠道、产品和服务

**3.客户分群的算法**

- 有监督学习：回归算法、决策树算法
- 聚类分析：k-means、分层聚类算法

### 迭代聚类法

**1.k-means 聚类过程**

- 设定 k 值（超参数），确定聚类数（软件随机分配聚类中心所需的种子）
- 计算每个记录到类中心的距离（欧式），并分成 k 类
- 然后把 k 类中心（均值），作为新的中心，重新计算距离
- 迭代到收敛标准停止（最小二乘准则）

**2.训练数据**

- 选择数据
- 初始化中心点
- 将离数据近的点划分到相应类
- 更新类的中心
- 重新将离数据近的点划分到相应类
- 反复进行上述两步直至不再有变化

**3.k-means 聚类要点**

- 预先处理变量的缺失值、异常值
- 变量标准化
- 不同维度的变量，相关性尽量低
- 如何决定合适的分群个数？（k 一般先从 3~10 尝试）

  - 主要推荐轮廓系数法（样本量最好 2000 以下，采用抽样方法；此方法不适合大样本）
  - 轮廓系数值介于[-1,1]，趋于 1 代表内聚性和分离度最好

**4.快速聚类的两种运用背景**

- 发现异常情况（原始数据直接使用聚类就是做异常情况）
- 将个案数据做划分（客户分群；计算原始变量的百分位秩、Turkey 正态打分、对数转换等进行异常的消除，之后做分群）

变量转换总结：

- 非对称变量在聚类分析中选用百分位秩和 Tukey 正态打分比较多
- 在回归分析中取对数比较多

**一般情境下的聚类**

变量归一化 → 分布转换 → 主成分 → 聚类

**发现异常情况的聚类**

变量归一化 → 主成分 → 聚类

### 聚类事后分析

分组命名

- 描述性统计
- 决策树的分组画像和规则

### 宏观业务指标预测框架

- 明确分析目的
- 根据业务理解做假设
- 做出指标预测
- 根据新假设做出指标预测
- 根据后期实际数据调整假设
- 最终预测结果

### 趋势分解法

- 趋势项
- 循环或者季节性
- 随机

**1.基本时间序列法**

- 逐步回归：有明显趋势
- 指数平滑法：无明显趋势
- 霍尔特-温特指数平滑法：有趋势，有季节或周期效应

**2.有季节效应的时间序列**

- 加法效用
- 乘法效应

### ARIMA 方法

**1.平稳时间序列**

**2.平稳时间序列模型设置与识别**

- 自相关函数（ACF）
- 偏自相关函数（PACF）

  - 是排除了其他变量的影响之后两个变量之间的相关系数
  - AR 模型中，ACF 失效。PACF 呈现指数递减。

- ARMA 的模型设定与识别

  - AR 模型：ACF 拖尾，PACF 截尾
  - MA 模型：相反
  - AIC、BIC 准则选择，越小越好

### 时间序列回归

**1.残差自相关检验（DW 检验）**

<!-- prettier-ignore-start -->
{% mathjax %}
DW=2(1-\bar{p})
{% endmathjax %}
<!-- prettier-ignore-end -->
